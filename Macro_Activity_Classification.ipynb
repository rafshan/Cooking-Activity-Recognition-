{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Macro_Activity_Without_Masking_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jPihmLbUvoa"
      },
      "source": [
        "# Train macro Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l47rNZZB6YJN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWzF5-ugUvoc"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "#import cv2\n",
        "import io\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shutil\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ztvv_ZZ6YJP"
      },
      "source": [
        "# Options to set if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCHJBndW6YJP"
      },
      "source": [
        "# from keras.backend.tensorflow_backend import set_session\n",
        "# import tensorflow as tf\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
        "# config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
        "# sess = tf.Session(config=config)\n",
        "# set_session(sess)  # set this TensorFlow session as the default "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9ZKOlv1PUVY",
        "outputId": "8c5e26c6-e6a3-4efd-e7d5-00ee754dc610"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = 'data_2.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDej6DfZUvol"
      },
      "source": [
        "data_dir_train='train'\n",
        "data_dir_val= 'val'\n",
        "data_dir_test= 'test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q87cb-fJUvoo"
      },
      "source": [
        "sub_dirs = ['left_hip','right_arm','right_wrist' ] #three sensors, 9 channels total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho3e4qQQUvor"
      },
      "source": [
        "def parse_IMU_files(parent_dir, sub_dirs, startTime, endTime, file_name, window_length):\n",
        "    \n",
        "    data = []\n",
        "    \n",
        "    data_count = 0\n",
        "    \n",
        "    for sub_dir in sub_dirs:\n",
        "        channel=[]\n",
        "        \n",
        "        for fn in glob.glob(os.path.join(parent_dir,sub_dir, file_name)):\n",
        "            file = open(fn, newline='')\n",
        "            reader = csv.reader(file)\n",
        "            first = True\n",
        "            count = 0\n",
        "            for row in reader:\n",
        "                \n",
        "                if first:\n",
        "                    first = False\n",
        "                    continue\n",
        "                \n",
        "                timestamp=float(row[3]) #4th column is timestamp\n",
        "                if timestamp >=startTime and timestamp <=endTime and count<window_length:\n",
        "                    \n",
        "                    channel.append([float(row[0]),float(row[1]),float(row[2])])\n",
        "                    count = count + 1 \n",
        "                    data_count = data_count+1\n",
        "                    \n",
        "        data.append(channel)         \n",
        "    return data, data_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7_dZqQ06YJU"
      },
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def parse_IMU_files_2(parent_dir, sub_dirs, startTime, endTime, file_name, window_length):\n",
        "    \n",
        "    data = []\n",
        "    \n",
        "    data_count = 0\n",
        "    for sub_dir in sub_dirs:\n",
        "        channel=[]\n",
        "        \n",
        "        \n",
        "        for fn in glob.glob(os.path.join(parent_dir,sub_dir, file_name)):\n",
        "            file = open(fn, newline='')\n",
        "            reader = csv.reader(file)\n",
        "            first = True\n",
        "            count = 0\n",
        "            for row in reader:\n",
        "                \n",
        "                if first:\n",
        "                    first = False\n",
        "                    continue\n",
        "                \n",
        "                timestamp=float(row[3]) #4th column is timestamp\n",
        "                \n",
        "                window_jitter1 = random.randint(-150,150)\n",
        "                \n",
        "                window_jitter2 = random.randint(-150,150)\n",
        "        \n",
        "                if timestamp >=(startTime+window_jitter1) and timestamp <=(endTime+window_jitter2) and count<window_length:\n",
        "                    \n",
        "                    channel.append([float(row[0]),float(row[1]),float(row[2])])\n",
        "                    count = count + 1  \n",
        "                    \n",
        "                    data_count = data_count+1\n",
        "                    \n",
        "        data.append(channel)         \n",
        "    return data, data_count\n",
        "\n",
        "\n",
        "def parse_IMU_files_3(parent_dir, sub_dirs, startTime, endTime, file_name, window_length):\n",
        "    \n",
        "    data = []\n",
        "    \n",
        "    data_count = 0\n",
        "    for sub_dir in sub_dirs:\n",
        "        channel=[]\n",
        "                \n",
        "        window_jitter1 = random.randint(-1500,1500)\n",
        "        window_jitter2 = random.randint(-1500,1500)\n",
        "            \n",
        "        for fn in glob.glob(os.path.join(parent_dir,sub_dir, file_name)):\n",
        "            file = open(fn, newline='')\n",
        "            reader = csv.reader(file)\n",
        "            first = True\n",
        "            count = 0\n",
        "            \n",
        "            for row in reader:\n",
        "                \n",
        "                if first:\n",
        "                    first = False\n",
        "                    continue\n",
        "                \n",
        "                timestamp=float(row[3]) #4th column is timestamp\n",
        "    \n",
        "                if timestamp >=(startTime+window_jitter1) and timestamp <=(endTime+window_jitter2) and count<window_length:\n",
        "                    \n",
        "                    channel.append([float(row[0]),float(row[1]),float(row[2])])\n",
        "                    count = count + 1  \n",
        "                    \n",
        "                    data_count = data_count+1\n",
        "                    \n",
        "        data.append(channel)         \n",
        "    return data, data_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ-DRBW-6YJV"
      },
      "source": [
        "data_min_count = 100 # per sample, valid points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLINn0siUvou"
      },
      "source": [
        "def get_train_data(data_dir, sub_dirs):\n",
        "    files = os.listdir(data_dir+'/left_hip')\n",
        "    number_of_samples = 500\n",
        "    \n",
        "    labels_macro =dict()\n",
        "    \n",
        "    labels_macro['sandwich'] = 0\n",
        "    labels_macro['fruitsalad'] = 1\n",
        "    labels_macro['cereal'] = 2\n",
        "    \n",
        "    \n",
        "    #read the labels\n",
        "    labels_loc = 'LabelTable.csv'\n",
        "    file_label = open(labels_loc, newline='')\n",
        "    label_reader = csv.reader(file_label)\n",
        "    file_label_mapping = dict()\n",
        "    \n",
        "    first = True\n",
        "    for row in label_reader:\n",
        "        \n",
        "        if first:\n",
        "            first = False\n",
        "            continue\n",
        "        \n",
        "        file_label_mapping[row[0]+'.csv'] = labels_macro[row[1]]\n",
        "        \n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    for f in files:\n",
        "        \n",
        "        st_index = 0\n",
        "        end_index = 30000\n",
        "        step = 1000 #overlapping window, step: 1000. \n",
        "        window_index = 10000 #6 second window\n",
        "        \n",
        "        print('reading file:',f)\n",
        "        f_name = f\n",
        "        \n",
        "        if f_name == '.DS_Store':\n",
        "            continue\n",
        "        \n",
        "        curr_label_file = file_label_mapping[f_name]\n",
        "        \n",
        "        while st_index+step < end_index:\n",
        "        \n",
        "            data, data_count = parse_IMU_files(data_dir, sub_dirs, st_index, st_index+window_index,  f, number_of_samples)\n",
        "            st_index = st_index+step\n",
        "            \n",
        "            if data_count<data_min_count:\n",
        "                continue\n",
        "            \n",
        "            train_data_sample  = np.zeros((9, number_of_samples))\n",
        "            train_data_label   = curr_label_file\n",
        "            for i in range(len(data)):\n",
        "                for j in range(len(data[i])):\n",
        "                    train_data_sample[i*3,j]=data[i][j][0]\n",
        "                    train_data_sample[i*3+1,j]=data[i][j][1]\n",
        "                    train_data_sample[i*3+2,j]=data[i][j][2]\n",
        "            \n",
        "            all_data.append(train_data_sample)\n",
        "            all_labels.append(train_data_label)\n",
        "            \n",
        "    return all_data, all_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi_Fgq7C6YJV"
      },
      "source": [
        "def get_train_data_2(data_dir, sub_dirs):\n",
        "    files = os.listdir(data_dir+'/left_hip')\n",
        "    number_of_samples = 500\n",
        "    \n",
        "    labels_macro =dict()\n",
        "    \n",
        "    labels_macro['sandwich'] = 0\n",
        "    labels_macro['fruitsalad'] = 1\n",
        "    labels_macro['cereal'] = 2\n",
        "    \n",
        "    \n",
        "    #read the labels\n",
        "    labels_loc = 'LabelTable.csv'\n",
        "    file_label = open(labels_loc, newline='')\n",
        "    label_reader = csv.reader(file_label)\n",
        "    file_label_mapping = dict()\n",
        "    \n",
        "    first = True\n",
        "    for row in label_reader:\n",
        "        \n",
        "        if first:\n",
        "            first = False\n",
        "            continue\n",
        "        \n",
        "        file_label_mapping[row[0]+'.csv'] = labels_macro[row[1]]\n",
        "        \n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    \n",
        "    all_counts = []\n",
        "    \n",
        "    for f in files:\n",
        "        \n",
        "        st_index = 0\n",
        "        end_index = 30000\n",
        "        step = 1000 #overlapping window, step \n",
        "        window_index = 10000 #6 second window\n",
        "        \n",
        "        print('reading file:',f)\n",
        "        f_name = f\n",
        "        \n",
        "        if f_name == '.DS_Store':\n",
        "            continue\n",
        "        \n",
        "        curr_label_file = file_label_mapping[f_name]\n",
        "        \n",
        "        while st_index+step < end_index:\n",
        "        \n",
        "            data, data_count = parse_IMU_files_2(data_dir, sub_dirs, st_index, st_index+window_index,  f, number_of_samples)\n",
        "            \n",
        "            st_index = st_index+step\n",
        "            \n",
        "            if data_count<data_min_count:\n",
        "                continue\n",
        "            \n",
        "            train_data_sample  = np.zeros((9, number_of_samples))\n",
        "            train_data_label   = curr_label_file\n",
        "            for i in range(len(data)):\n",
        "                for j in range(len(data[i])):\n",
        "                    train_data_sample[i*3,j]=data[i][j][0]\n",
        "                    train_data_sample[i*3+1,j]=data[i][j][1]\n",
        "                    train_data_sample[i*3+2,j]=data[i][j][2]\n",
        "            \n",
        "            all_data.append(train_data_sample)\n",
        "            all_labels.append(train_data_label)\n",
        "            \n",
        "            all_counts.append(data_count)\n",
        "            \n",
        "    return all_data, all_labels, all_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiCYQmWv6YJW"
      },
      "source": [
        "def get_train_data_3(data_dir, sub_dirs):\n",
        "    files = os.listdir(data_dir+'/left_hip')\n",
        "    number_of_samples = 500\n",
        "    \n",
        "    labels_macro =dict()\n",
        "    \n",
        "    labels_macro['sandwich'] = 0\n",
        "    labels_macro['fruitsalad'] = 1\n",
        "    labels_macro['cereal'] = 2\n",
        "    \n",
        "    \n",
        "    #read the labels\n",
        "    labels_loc = 'LabelTable.csv'\n",
        "    file_label = open(labels_loc, newline='')\n",
        "    label_reader = csv.reader(file_label)\n",
        "    file_label_mapping = dict()\n",
        "    \n",
        "    first = True\n",
        "    for row in label_reader:\n",
        "        \n",
        "        if first:\n",
        "            first = False\n",
        "            continue\n",
        "        \n",
        "        file_label_mapping[row[0]+'.csv'] = labels_macro[row[1]]\n",
        "        \n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    \n",
        "    all_counts = []\n",
        "    \n",
        "    for f in files:\n",
        "        \n",
        "        st_index = 0\n",
        "        end_index = 30000\n",
        "        step = 1000 #overlapping window, step: 1000. \n",
        "        window_index = 10000 #6 second window\n",
        "        \n",
        "        print('reading file:',f)\n",
        "        f_name = f\n",
        "        \n",
        "        if f_name == '.DS_Store':\n",
        "            continue\n",
        "        \n",
        "        curr_label_file = file_label_mapping[f_name]\n",
        "        \n",
        "        while st_index+step < end_index:\n",
        "                \n",
        "            \n",
        "            data, data_count = parse_IMU_files_3(data_dir, sub_dirs, st_index, st_index+window_index,  f, number_of_samples)\n",
        "            \n",
        "            st_index = st_index+step\n",
        "            \n",
        "            if data_count<data_min_count:\n",
        "                continue\n",
        "            \n",
        "            train_data_sample  = np.zeros((9, number_of_samples))\n",
        "            train_data_label   = curr_label_file\n",
        "            for i in range(len(data)):\n",
        "                for j in range(len(data[i])):\n",
        "                    train_data_sample[i*3,j]=data[i][j][0]\n",
        "                    train_data_sample[i*3+1,j]=data[i][j][1]\n",
        "                    train_data_sample[i*3+2,j]=data[i][j][2]\n",
        "            \n",
        "            all_data.append(train_data_sample)\n",
        "            all_labels.append(train_data_label)\n",
        "            \n",
        "            all_counts.append(data_count)\n",
        "            \n",
        "            \n",
        "            \n",
        "    return all_data, all_labels, all_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEYY86nqUvoy",
        "scrolled": true,
        "outputId": "696b55a5-095d-4eb0-ab02-3b7c36c16749"
      },
      "source": [
        "#receive windowed training and validation data\n",
        "train_x, train_y = get_train_data(data_dir_train,sub_dirs)\n",
        "val_x, val_y = get_train_data(data_dir_val,sub_dirs)\n",
        "test_x, test_y = get_train_data(data_dir_test,sub_dirs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading file: subject2_file_642.csv\n",
            "reading file: subject2_file_724.csv\n",
            "reading file: subject2_file_368.csv\n",
            "reading file: subject1_file_871.csv\n",
            "reading file: subject2_file_101.csv\n",
            "reading file: subject3_file_260.csv\n",
            "reading file: subject3_file_531.csv\n",
            "reading file: subject3_file_388.csv\n",
            "reading file: subject1_file_45.csv\n",
            "reading file: subject2_file_917.csv\n",
            "reading file: subject2_file_709.csv\n",
            "reading file: subject3_file_571.csv\n",
            "reading file: subject3_file_609.csv\n",
            "reading file: subject2_file_218.csv\n",
            "reading file: subject3_file_377.csv\n",
            "reading file: subject2_file_92.csv\n",
            "reading file: subject2_file_840.csv\n",
            "reading file: subject1_file_287.csv\n",
            "reading file: subject2_file_847.csv\n",
            "reading file: subject1_file_39.csv\n",
            "reading file: subject2_file_52.csv\n",
            "reading file: subject2_file_749.csv\n",
            "reading file: subject1_file_385.csv\n",
            "reading file: subject3_file_111.csv\n",
            "reading file: subject3_file_753.csv\n",
            "reading file: subject3_file_62.csv\n",
            "reading file: subject1_file_281.csv\n",
            "reading file: subject3_file_251.csv\n",
            "reading file: subject1_file_830.csv\n",
            "reading file: subject3_file_520.csv\n",
            "reading file: subject3_file_562.csv\n",
            "reading file: subject3_file_328.csv\n",
            "reading file: subject2_file_792.csv\n",
            "reading file: subject2_file_517.csv\n",
            "reading file: subject1_file_526.csv\n",
            "reading file: subject2_file_838.csv\n",
            "reading file: subject1_file_685.csv\n",
            "reading file: subject1_file_213.csv\n",
            "reading file: subject2_file_880.csv\n",
            "reading file: subject1_file_243.csv\n",
            "reading file: subject2_file_157.csv\n",
            "reading file: subject2_file_926.csv\n",
            "reading file: subject1_file_979.csv\n",
            "reading file: subject3_file_854.csv\n",
            "reading file: subject1_file_968.csv\n",
            "reading file: subject1_file_204.csv\n",
            "reading file: subject3_file_718.csv\n",
            "reading file: subject2_file_581.csv\n",
            "reading file: subject2_file_325.csv\n",
            "reading file: subject1_file_247.csv\n",
            "reading file: subject2_file_674.csv\n",
            "reading file: subject3_file_554.csv\n",
            "reading file: subject1_file_485.csv\n",
            "reading file: subject2_file_89.csv\n",
            "reading file: subject3_file_99.csv\n",
            "reading file: subject3_file_901.csv\n",
            "reading file: subject3_file_818.csv\n",
            "reading file: subject2_file_248.csv\n",
            "reading file: subject1_file_957.csv\n",
            "reading file: subject2_file_541.csv\n",
            "reading file: subject2_file_49.csv\n",
            "reading file: subject3_file_318.csv\n",
            "reading file: subject2_file_831.csv\n",
            "reading file: subject2_file_485.csv\n",
            "reading file: subject2_file_980.csv\n",
            "reading file: subject3_file_345.csv\n",
            "reading file: subject1_file_37.csv\n",
            "reading file: subject2_file_679.csv\n",
            "reading file: subject3_file_146.csv\n",
            "reading file: subject3_file_853.csv\n",
            "reading file: subject1_file_784.csv\n",
            "reading file: subject2_file_635.csv\n",
            "reading file: subject2_file_821.csv\n",
            "reading file: subject2_file_641.csv\n",
            "reading file: subject3_file_526.csv\n",
            "reading file: subject1_file_309.csv\n",
            "reading file: subject2_file_425.csv\n",
            "reading file: subject3_file_580.csv\n",
            "reading file: subject2_file_74.csv\n",
            "reading file: subject2_file_624.csv\n",
            "reading file: subject2_file_500.csv\n",
            "reading file: subject3_file_9.csv\n",
            "reading file: subject2_file_721.csv\n",
            "reading file: subject2_file_314.csv\n",
            "reading file: subject2_file_95.csv\n",
            "reading file: subject3_file_197.csv\n",
            "reading file: subject1_file_496.csv\n",
            "reading file: subject1_file_853.csv\n",
            "reading file: subject3_file_511.csv\n",
            "reading file: subject3_file_85.csv\n",
            "reading file: subject3_file_352.csv\n",
            "reading file: subject3_file_61.csv\n",
            "reading file: subject1_file_430.csv\n",
            "reading file: subject1_file_713.csv\n",
            "reading file: subject3_file_807.csv\n",
            "reading file: subject2_file_336.csv\n",
            "reading file: subject1_file_463.csv\n",
            "reading file: subject1_file_969.csv\n",
            "reading file: subject3_file_74.csv\n",
            "reading file: subject2_file_923.csv\n",
            "reading file: subject3_file_355.csv\n",
            "reading file: subject2_file_402.csv\n",
            "reading file: subject2_file_308.csv\n",
            "reading file: subject2_file_549.csv\n",
            "reading file: subject1_file_96.csv\n",
            "reading file: subject1_file_958.csv\n",
            "reading file: subject2_file_592.csv\n",
            "reading file: subject2_file_832.csv\n",
            "reading file: subject2_file_358.csv\n",
            "reading file: subject3_file_362.csv\n",
            "reading file: subject1_file_238.csv\n",
            "reading file: subject3_file_315.csv\n",
            "reading file: subject3_file_88.csv\n",
            "reading file: subject2_file_942.csv\n",
            "reading file: subject2_file_202.csv\n",
            "reading file: subject2_file_263.csv\n",
            "reading file: subject2_file_85.csv\n",
            "reading file: subject2_file_681.csv\n",
            "reading file: subject1_file_808.csv\n",
            "reading file: subject2_file_616.csv\n",
            "reading file: subject3_file_67.csv\n",
            "reading file: subject3_file_402.csv\n",
            "reading file: subject1_file_424.csv\n",
            "reading file: subject1_file_770.csv\n",
            "reading file: subject1_file_348.csv\n",
            "reading file: subject3_file_867.csv\n",
            "reading file: subject1_file_319.csv\n",
            "reading file: subject3_file_808.csv\n",
            "reading file: subject1_file_684.csv\n",
            "reading file: subject3_file_817.csv\n",
            "reading file: subject2_file_257.csv\n",
            "reading file: subject1_file_211.csv\n",
            "reading file: subject2_file_526.csv\n",
            "reading file: subject1_file_768.csv\n",
            "reading file: subject1_file_162.csv\n",
            "reading file: subject2_file_96.csv\n",
            "reading file: subject2_file_175.csv\n",
            "reading file: subject3_file_973.csv\n",
            "reading file: subject3_file_364.csv\n",
            "reading file: subject3_file_537.csv\n",
            "reading file: subject2_file_464.csv\n",
            "reading file: subject1_file_734.csv\n",
            "reading file: subject2_file_803.csv\n",
            "reading file: subject2_file_457.csv\n",
            "reading file: subject3_file_557.csv\n",
            "reading file: subject2_file_25.csv\n",
            "reading file: subject1_file_536.csv\n",
            "reading file: subject3_file_684.csv\n",
            "reading file: subject1_file_807.csv\n",
            "reading file: subject3_file_231.csv\n",
            "reading file: subject3_file_382.csv\n",
            "reading file: subject2_file_172.csv\n",
            "reading file: subject1_file_310.csv\n",
            "reading file: subject3_file_540.csv\n",
            "reading file: subject3_file_695.csv\n",
            "reading file: subject3_file_607.csv\n",
            "reading file: subject3_file_999.csv\n",
            "reading file: subject3_file_849.csv\n",
            "reading file: subject1_file_302.csv\n",
            "reading file: subject1_file_814.csv\n",
            "reading file: subject3_file_97.csv\n",
            "reading file: subject2_file_770.csv\n",
            "reading file: subject1_file_743.csv\n",
            "reading file: subject1_file_272.csv\n",
            "reading file: subject1_file_622.csv\n",
            "reading file: subject3_file_949.csv\n",
            "reading file: subject2_file_588.csv\n",
            "reading file: subject1_file_140.csv\n",
            "reading file: subject3_file_207.csv\n",
            "reading file: subject1_file_980.csv\n",
            "reading file: subject2_file_114.csv\n",
            "reading file: subject3_file_547.csv\n",
            "reading file: subject1_file_417.csv\n",
            "reading file: subject2_file_540.csv\n",
            "reading file: subject3_file_931.csv\n",
            "reading file: subject3_file_14.csv\n",
            "reading file: subject1_file_976.csv\n",
            "reading file: subject1_file_254.csv\n",
            "reading file: subject1_file_513.csv\n",
            "reading file: subject2_file_123.csv\n",
            "reading file: subject1_file_618.csv\n",
            "reading file: subject2_file_746.csv\n",
            "reading file: subject2_file_37.csv\n",
            "reading file: subject1_file_320.csv\n",
            "reading file: subject3_file_216.csv\n",
            "reading file: subject3_file_415.csv\n",
            "reading file: subject1_file_570.csv\n",
            "reading file: subject1_file_849.csv\n",
            "reading file: subject2_file_775.csv\n",
            "reading file: subject2_file_565.csv\n",
            "reading file: subject1_file_869.csv\n",
            "reading file: subject1_file_858.csv\n",
            "reading file: subject3_file_462.csv\n",
            "reading file: subject2_file_495.csv\n",
            "reading file: subject3_file_610.csv\n",
            "reading file: subject3_file_935.csv\n",
            "reading file: subject3_file_335.csv\n",
            "reading file: subject2_file_295.csv\n",
            "reading file: subject3_file_640.csv\n",
            "reading file: subject3_file_56.csv\n",
            "reading file: subject3_file_43.csv\n",
            "reading file: subject2_file_723.csv\n",
            "reading file: subject1_file_721.csv\n",
            "reading file: subject2_file_658.csv\n",
            "reading file: subject2_file_17.csv\n",
            "reading file: subject2_file_344.csv\n",
            "reading file: subject3_file_884.csv\n",
            "reading file: subject2_file_650.csv\n",
            "reading file: subject1_file_263.csv\n",
            "reading file: subject1_file_499.csv\n",
            "reading file: subject2_file_1.csv\n",
            "reading file: subject2_file_129.csv\n",
            "reading file: subject3_file_264.csv\n",
            "reading file: subject3_file_491.csv\n",
            "reading file: subject2_file_522.csv\n",
            "reading file: subject3_file_11.csv\n",
            "reading file: subject1_file_884.csv\n",
            "reading file: subject1_file_98.csv\n",
            "reading file: subject2_file_199.csv\n",
            "reading file: subject1_file_786.csv\n",
            "reading file: subject3_file_34.csv\n",
            "reading file: subject3_file_951.csv\n",
            "reading file: subject3_file_135.csv\n",
            "reading file: subject3_file_850.csv\n",
            "reading file: subject1_file_313.csv\n",
            "reading file: subject3_file_435.csv\n",
            "reading file: subject1_file_564.csv\n",
            "reading file: subject2_file_341.csv\n",
            "reading file: subject1_file_964.csv\n",
            "reading file: subject2_file_872.csv\n",
            "reading file: subject2_file_328.csv\n",
            "reading file: subject3_file_507.csv\n",
            "reading file: subject2_file_899.csv\n",
            "reading file: subject3_file_775.csv\n",
            "reading file: subject2_file_731.csv\n",
            "reading file: subject3_file_977.csv\n",
            "reading file: subject2_file_452.csv\n",
            "reading file: subject3_file_391.csv\n",
            "reading file: subject1_file_72.csv\n",
            "reading file: subject2_file_180.csv\n",
            "reading file: subject2_file_646.csv\n",
            "reading file: subject1_file_389.csv\n",
            "reading file: subject2_file_324.csv\n",
            "reading file: subject2_file_204.csv\n",
            "reading file: subject3_file_974.csv\n",
            "reading file: subject2_file_436.csv\n",
            "reading file: subject2_file_743.csv\n",
            "reading file: subject3_file_991.csv\n",
            "reading file: subject3_file_36.csv\n",
            "reading file: subject1_file_462.csv\n",
            "reading file: subject2_file_409.csv\n",
            "reading file: subject1_file_873.csv\n",
            "reading file: subject3_file_893.csv\n",
            "reading file: subject3_file_246.csv\n",
            "reading file: subject2_file_763.csv\n",
            "reading file: subject3_file_829.csv\n",
            "reading file: subject1_file_780.csv\n",
            "reading file: subject3_file_855.csv\n",
            "reading file: subject3_file_749.csv\n",
            "reading file: subject2_file_171.csv\n",
            "reading file: subject2_file_4.csv\n",
            "reading file: subject1_file_11.csv\n",
            "reading file: subject3_file_389.csv\n",
            "reading file: subject2_file_378.csv\n",
            "reading file: subject3_file_823.csv\n",
            "reading file: subject3_file_10.csv\n",
            "reading file: subject2_file_586.csv\n",
            "reading file: subject2_file_138.csv\n",
            "reading file: subject1_file_588.csv\n",
            "reading file: subject3_file_827.csv\n",
            "reading file: subject1_file_353.csv\n",
            "reading file: subject2_file_453.csv\n",
            "reading file: subject1_file_985.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3AkKGdWY6YJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f0f724-eced-442c-efd4-29885424b27f"
      },
      "source": [
        "train_x2, train_y2, all_counts = get_train_data_2(data_dir_train,sub_dirs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading file: subject2_file_642.csv\n",
            "reading file: subject2_file_724.csv\n",
            "reading file: subject2_file_368.csv\n",
            "reading file: subject1_file_871.csv\n",
            "reading file: subject2_file_101.csv\n",
            "reading file: subject3_file_260.csv\n",
            "reading file: subject3_file_531.csv\n",
            "reading file: subject3_file_388.csv\n",
            "reading file: subject1_file_45.csv\n",
            "reading file: subject2_file_917.csv\n",
            "reading file: subject2_file_709.csv\n",
            "reading file: subject3_file_571.csv\n",
            "reading file: subject3_file_609.csv\n",
            "reading file: subject2_file_218.csv\n",
            "reading file: subject3_file_377.csv\n",
            "reading file: subject2_file_92.csv\n",
            "reading file: subject2_file_840.csv\n",
            "reading file: subject1_file_287.csv\n",
            "reading file: subject2_file_847.csv\n",
            "reading file: subject1_file_39.csv\n",
            "reading file: subject2_file_52.csv\n",
            "reading file: subject2_file_749.csv\n",
            "reading file: subject1_file_385.csv\n",
            "reading file: subject3_file_111.csv\n",
            "reading file: subject3_file_753.csv\n",
            "reading file: subject3_file_62.csv\n",
            "reading file: subject1_file_281.csv\n",
            "reading file: subject3_file_251.csv\n",
            "reading file: subject1_file_830.csv\n",
            "reading file: subject3_file_520.csv\n",
            "reading file: subject3_file_562.csv\n",
            "reading file: subject3_file_328.csv\n",
            "reading file: subject2_file_792.csv\n",
            "reading file: subject2_file_517.csv\n",
            "reading file: subject1_file_526.csv\n",
            "reading file: subject2_file_838.csv\n",
            "reading file: subject1_file_685.csv\n",
            "reading file: subject1_file_213.csv\n",
            "reading file: subject2_file_880.csv\n",
            "reading file: subject1_file_243.csv\n",
            "reading file: subject2_file_157.csv\n",
            "reading file: subject2_file_926.csv\n",
            "reading file: subject1_file_979.csv\n",
            "reading file: subject3_file_854.csv\n",
            "reading file: subject1_file_968.csv\n",
            "reading file: subject1_file_204.csv\n",
            "reading file: subject3_file_718.csv\n",
            "reading file: subject2_file_581.csv\n",
            "reading file: subject2_file_325.csv\n",
            "reading file: subject1_file_247.csv\n",
            "reading file: subject2_file_674.csv\n",
            "reading file: subject3_file_554.csv\n",
            "reading file: subject1_file_485.csv\n",
            "reading file: subject2_file_89.csv\n",
            "reading file: subject3_file_99.csv\n",
            "reading file: subject3_file_901.csv\n",
            "reading file: subject3_file_818.csv\n",
            "reading file: subject2_file_248.csv\n",
            "reading file: subject1_file_957.csv\n",
            "reading file: subject2_file_541.csv\n",
            "reading file: subject2_file_49.csv\n",
            "reading file: subject3_file_318.csv\n",
            "reading file: subject2_file_831.csv\n",
            "reading file: subject2_file_485.csv\n",
            "reading file: subject2_file_980.csv\n",
            "reading file: subject3_file_345.csv\n",
            "reading file: subject1_file_37.csv\n",
            "reading file: subject2_file_679.csv\n",
            "reading file: subject3_file_146.csv\n",
            "reading file: subject3_file_853.csv\n",
            "reading file: subject1_file_784.csv\n",
            "reading file: subject2_file_635.csv\n",
            "reading file: subject2_file_821.csv\n",
            "reading file: subject2_file_641.csv\n",
            "reading file: subject3_file_526.csv\n",
            "reading file: subject1_file_309.csv\n",
            "reading file: subject2_file_425.csv\n",
            "reading file: subject3_file_580.csv\n",
            "reading file: subject2_file_74.csv\n",
            "reading file: subject2_file_624.csv\n",
            "reading file: subject2_file_500.csv\n",
            "reading file: subject3_file_9.csv\n",
            "reading file: subject2_file_721.csv\n",
            "reading file: subject2_file_314.csv\n",
            "reading file: subject2_file_95.csv\n",
            "reading file: subject3_file_197.csv\n",
            "reading file: subject1_file_496.csv\n",
            "reading file: subject1_file_853.csv\n",
            "reading file: subject3_file_511.csv\n",
            "reading file: subject3_file_85.csv\n",
            "reading file: subject3_file_352.csv\n",
            "reading file: subject3_file_61.csv\n",
            "reading file: subject1_file_430.csv\n",
            "reading file: subject1_file_713.csv\n",
            "reading file: subject3_file_807.csv\n",
            "reading file: subject2_file_336.csv\n",
            "reading file: subject1_file_463.csv\n",
            "reading file: subject1_file_969.csv\n",
            "reading file: subject3_file_74.csv\n",
            "reading file: subject2_file_923.csv\n",
            "reading file: subject3_file_355.csv\n",
            "reading file: subject2_file_402.csv\n",
            "reading file: subject2_file_308.csv\n",
            "reading file: subject2_file_549.csv\n",
            "reading file: subject1_file_96.csv\n",
            "reading file: subject1_file_958.csv\n",
            "reading file: subject2_file_592.csv\n",
            "reading file: subject2_file_832.csv\n",
            "reading file: subject2_file_358.csv\n",
            "reading file: subject3_file_362.csv\n",
            "reading file: subject1_file_238.csv\n",
            "reading file: subject3_file_315.csv\n",
            "reading file: subject3_file_88.csv\n",
            "reading file: subject2_file_942.csv\n",
            "reading file: subject2_file_202.csv\n",
            "reading file: subject2_file_263.csv\n",
            "reading file: subject2_file_85.csv\n",
            "reading file: subject2_file_681.csv\n",
            "reading file: subject1_file_808.csv\n",
            "reading file: subject2_file_616.csv\n",
            "reading file: subject3_file_67.csv\n",
            "reading file: subject3_file_402.csv\n",
            "reading file: subject1_file_424.csv\n",
            "reading file: subject1_file_770.csv\n",
            "reading file: subject1_file_348.csv\n",
            "reading file: subject3_file_867.csv\n",
            "reading file: subject1_file_319.csv\n",
            "reading file: subject3_file_808.csv\n",
            "reading file: subject1_file_684.csv\n",
            "reading file: subject3_file_817.csv\n",
            "reading file: subject2_file_257.csv\n",
            "reading file: subject1_file_211.csv\n",
            "reading file: subject2_file_526.csv\n",
            "reading file: subject1_file_768.csv\n",
            "reading file: subject1_file_162.csv\n",
            "reading file: subject2_file_96.csv\n",
            "reading file: subject2_file_175.csv\n",
            "reading file: subject3_file_973.csv\n",
            "reading file: subject3_file_364.csv\n",
            "reading file: subject3_file_537.csv\n",
            "reading file: subject2_file_464.csv\n",
            "reading file: subject1_file_734.csv\n",
            "reading file: subject2_file_803.csv\n",
            "reading file: subject2_file_457.csv\n",
            "reading file: subject3_file_557.csv\n",
            "reading file: subject2_file_25.csv\n",
            "reading file: subject1_file_536.csv\n",
            "reading file: subject3_file_684.csv\n",
            "reading file: subject1_file_807.csv\n",
            "reading file: subject3_file_231.csv\n",
            "reading file: subject3_file_382.csv\n",
            "reading file: subject2_file_172.csv\n",
            "reading file: subject1_file_310.csv\n",
            "reading file: subject3_file_540.csv\n",
            "reading file: subject3_file_695.csv\n",
            "reading file: subject3_file_607.csv\n",
            "reading file: subject3_file_999.csv\n",
            "reading file: subject3_file_849.csv\n",
            "reading file: subject1_file_302.csv\n",
            "reading file: subject1_file_814.csv\n",
            "reading file: subject3_file_97.csv\n",
            "reading file: subject2_file_770.csv\n",
            "reading file: subject1_file_743.csv\n",
            "reading file: subject1_file_272.csv\n",
            "reading file: subject1_file_622.csv\n",
            "reading file: subject3_file_949.csv\n",
            "reading file: subject2_file_588.csv\n",
            "reading file: subject1_file_140.csv\n",
            "reading file: subject3_file_207.csv\n",
            "reading file: subject1_file_980.csv\n",
            "reading file: subject2_file_114.csv\n",
            "reading file: subject3_file_547.csv\n",
            "reading file: subject1_file_417.csv\n",
            "reading file: subject2_file_540.csv\n",
            "reading file: subject3_file_931.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "98AzLn0H6YJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea5f0c6-f88c-442f-ec3e-e9c7104ab59b"
      },
      "source": [
        "train_x3, train_y3, all_counts = get_train_data_3(data_dir_train,sub_dirs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading file: subject2_file_642.csv\n",
            "reading file: subject2_file_724.csv\n",
            "reading file: subject2_file_368.csv\n",
            "reading file: subject1_file_871.csv\n",
            "reading file: subject2_file_101.csv\n",
            "reading file: subject3_file_260.csv\n",
            "reading file: subject3_file_531.csv\n",
            "reading file: subject3_file_388.csv\n",
            "reading file: subject1_file_45.csv\n",
            "reading file: subject2_file_917.csv\n",
            "reading file: subject2_file_709.csv\n",
            "reading file: subject3_file_571.csv\n",
            "reading file: subject3_file_609.csv\n",
            "reading file: subject2_file_218.csv\n",
            "reading file: subject3_file_377.csv\n",
            "reading file: subject2_file_92.csv\n",
            "reading file: subject2_file_840.csv\n",
            "reading file: subject1_file_287.csv\n",
            "reading file: subject2_file_847.csv\n",
            "reading file: subject1_file_39.csv\n",
            "reading file: subject2_file_52.csv\n",
            "reading file: subject2_file_749.csv\n",
            "reading file: subject1_file_385.csv\n",
            "reading file: subject3_file_111.csv\n",
            "reading file: subject3_file_753.csv\n",
            "reading file: subject3_file_62.csv\n",
            "reading file: subject1_file_281.csv\n",
            "reading file: subject3_file_251.csv\n",
            "reading file: subject1_file_830.csv\n",
            "reading file: subject3_file_520.csv\n",
            "reading file: subject3_file_562.csv\n",
            "reading file: subject3_file_328.csv\n",
            "reading file: subject2_file_792.csv\n",
            "reading file: subject2_file_517.csv\n",
            "reading file: subject1_file_526.csv\n",
            "reading file: subject2_file_838.csv\n",
            "reading file: subject1_file_685.csv\n",
            "reading file: subject1_file_213.csv\n",
            "reading file: subject2_file_880.csv\n",
            "reading file: subject1_file_243.csv\n",
            "reading file: subject2_file_157.csv\n",
            "reading file: subject2_file_926.csv\n",
            "reading file: subject1_file_979.csv\n",
            "reading file: subject3_file_854.csv\n",
            "reading file: subject1_file_968.csv\n",
            "reading file: subject1_file_204.csv\n",
            "reading file: subject3_file_718.csv\n",
            "reading file: subject2_file_581.csv\n",
            "reading file: subject2_file_325.csv\n",
            "reading file: subject1_file_247.csv\n",
            "reading file: subject2_file_674.csv\n",
            "reading file: subject3_file_554.csv\n",
            "reading file: subject1_file_485.csv\n",
            "reading file: subject2_file_89.csv\n",
            "reading file: subject3_file_99.csv\n",
            "reading file: subject3_file_901.csv\n",
            "reading file: subject3_file_818.csv\n",
            "reading file: subject2_file_248.csv\n",
            "reading file: subject1_file_957.csv\n",
            "reading file: subject2_file_541.csv\n",
            "reading file: subject2_file_49.csv\n",
            "reading file: subject3_file_318.csv\n",
            "reading file: subject2_file_831.csv\n",
            "reading file: subject2_file_485.csv\n",
            "reading file: subject2_file_980.csv\n",
            "reading file: subject3_file_345.csv\n",
            "reading file: subject1_file_37.csv\n",
            "reading file: subject2_file_679.csv\n",
            "reading file: subject3_file_146.csv\n",
            "reading file: subject3_file_853.csv\n",
            "reading file: subject1_file_784.csv\n",
            "reading file: subject2_file_635.csv\n",
            "reading file: subject2_file_821.csv\n",
            "reading file: subject2_file_641.csv\n",
            "reading file: subject3_file_526.csv\n",
            "reading file: subject1_file_309.csv\n",
            "reading file: subject2_file_425.csv\n",
            "reading file: subject3_file_580.csv\n",
            "reading file: subject2_file_74.csv\n",
            "reading file: subject2_file_624.csv\n",
            "reading file: subject2_file_500.csv\n",
            "reading file: subject3_file_9.csv\n",
            "reading file: subject2_file_721.csv\n",
            "reading file: subject2_file_314.csv\n",
            "reading file: subject2_file_95.csv\n",
            "reading file: subject3_file_197.csv\n",
            "reading file: subject1_file_496.csv\n",
            "reading file: subject1_file_853.csv\n",
            "reading file: subject3_file_511.csv\n",
            "reading file: subject3_file_85.csv\n",
            "reading file: subject3_file_352.csv\n",
            "reading file: subject3_file_61.csv\n",
            "reading file: subject1_file_430.csv\n",
            "reading file: subject1_file_713.csv\n",
            "reading file: subject3_file_807.csv\n",
            "reading file: subject2_file_336.csv\n",
            "reading file: subject1_file_463.csv\n",
            "reading file: subject1_file_969.csv\n",
            "reading file: subject3_file_74.csv\n",
            "reading file: subject2_file_923.csv\n",
            "reading file: subject3_file_355.csv\n",
            "reading file: subject2_file_402.csv\n",
            "reading file: subject2_file_308.csv\n",
            "reading file: subject2_file_549.csv\n",
            "reading file: subject1_file_96.csv\n",
            "reading file: subject1_file_958.csv\n",
            "reading file: subject2_file_592.csv\n",
            "reading file: subject2_file_832.csv\n",
            "reading file: subject2_file_358.csv\n",
            "reading file: subject3_file_362.csv\n",
            "reading file: subject1_file_238.csv\n",
            "reading file: subject3_file_315.csv\n",
            "reading file: subject3_file_88.csv\n",
            "reading file: subject2_file_942.csv\n",
            "reading file: subject2_file_202.csv\n",
            "reading file: subject2_file_263.csv\n",
            "reading file: subject2_file_85.csv\n",
            "reading file: subject2_file_681.csv\n",
            "reading file: subject1_file_808.csv\n",
            "reading file: subject2_file_616.csv\n",
            "reading file: subject3_file_67.csv\n",
            "reading file: subject3_file_402.csv\n",
            "reading file: subject1_file_424.csv\n",
            "reading file: subject1_file_770.csv\n",
            "reading file: subject1_file_348.csv\n",
            "reading file: subject3_file_867.csv\n",
            "reading file: subject1_file_319.csv\n",
            "reading file: subject3_file_808.csv\n",
            "reading file: subject1_file_684.csv\n",
            "reading file: subject3_file_817.csv\n",
            "reading file: subject2_file_257.csv\n",
            "reading file: subject1_file_211.csv\n",
            "reading file: subject2_file_526.csv\n",
            "reading file: subject1_file_768.csv\n",
            "reading file: subject1_file_162.csv\n",
            "reading file: subject2_file_96.csv\n",
            "reading file: subject2_file_175.csv\n",
            "reading file: subject3_file_973.csv\n",
            "reading file: subject3_file_364.csv\n",
            "reading file: subject3_file_537.csv\n",
            "reading file: subject2_file_464.csv\n",
            "reading file: subject1_file_734.csv\n",
            "reading file: subject2_file_803.csv\n",
            "reading file: subject2_file_457.csv\n",
            "reading file: subject3_file_557.csv\n",
            "reading file: subject2_file_25.csv\n",
            "reading file: subject1_file_536.csv\n",
            "reading file: subject3_file_684.csv\n",
            "reading file: subject1_file_807.csv\n",
            "reading file: subject3_file_231.csv\n",
            "reading file: subject3_file_382.csv\n",
            "reading file: subject2_file_172.csv\n",
            "reading file: subject1_file_310.csv\n",
            "reading file: subject3_file_540.csv\n",
            "reading file: subject3_file_695.csv\n",
            "reading file: subject3_file_607.csv\n",
            "reading file: subject3_file_999.csv\n",
            "reading file: subject3_file_849.csv\n",
            "reading file: subject1_file_302.csv\n",
            "reading file: subject1_file_814.csv\n",
            "reading file: subject3_file_97.csv\n",
            "reading file: subject2_file_770.csv\n",
            "reading file: subject1_file_743.csv\n",
            "reading file: subject1_file_272.csv\n",
            "reading file: subject1_file_622.csv\n",
            "reading file: subject3_file_949.csv\n",
            "reading file: subject2_file_588.csv\n",
            "reading file: subject1_file_140.csv\n",
            "reading file: subject3_file_207.csv\n",
            "reading file: subject1_file_980.csv\n",
            "reading file: subject2_file_114.csv\n",
            "reading file: subject3_file_547.csv\n",
            "reading file: subject1_file_417.csv\n",
            "reading file: subject2_file_540.csv\n",
            "reading file: subject3_file_931.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMkkAffs6YJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06a5f56-a0df-403c-cced-af3b77ab3eae"
      },
      "source": [
        "train_samples_2 = np.array(train_x2) \n",
        "train_labels2_2 = np.array(train_y2)\n",
        "print(train_samples_2.shape)\n",
        "print(train_labels2_2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4863, 9, 500)\n",
            "(4863,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNXgaVs66YJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9fe3a4-16d3-44c6-d5e7-6a615ee6813a"
      },
      "source": [
        "train_samples_3 = np.array(train_x3) \n",
        "train_labels2_3 = np.array(train_y3)\n",
        "print(train_samples_3.shape)\n",
        "print(train_labels2_3.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4863, 9, 500)\n",
            "(4863,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2w_y9dmUvo1"
      },
      "source": [
        "train_samples = np.array(train_x) \n",
        "train_labels2 = np.array(train_y)\n",
        "val_samples = np.array(val_x) \n",
        "val_labels2 = np.array(val_y)\n",
        "test_samples = np.array(test_x) \n",
        "test_labels2 = np.array(test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QgKGfXc6YJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e248766-b6f8-41e5-b71e-eb706d488d40"
      },
      "source": [
        "print(train_samples.shape)\n",
        "print(train_labels2.shape)\n",
        "print(val_samples.shape)\n",
        "print(val_labels2.shape)\n",
        "print(test_samples.shape)\n",
        "print(test_labels2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4864, 9, 500)\n",
            "(4864,)\n",
            "(1348, 9, 500)\n",
            "(1348,)\n",
            "(1356, 9, 500)\n",
            "(1356,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdUwD-4W6YJl"
      },
      "source": [
        "train_samples = np.vstack((train_samples,train_samples_2, train_samples_3))\n",
        "train_labels2 = np.hstack((train_labels2, train_labels2_2, train_labels2_3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNUKXbk9Uvo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9deee81-413f-40e5-ee16-0d71c12bd34e"
      },
      "source": [
        "print(train_samples.shape)\n",
        "print(train_labels2.shape)\n",
        "print(val_samples.shape)\n",
        "print(val_labels2.shape)\n",
        "print(test_samples.shape)\n",
        "print(test_labels2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14590, 9, 500)\n",
            "(14590,)\n",
            "(1348, 9, 500)\n",
            "(1348,)\n",
            "(1356, 9, 500)\n",
            "(1356,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1OiMsBOUvo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1555370a-687f-48e2-bd6d-afd9ae7e1714"
      },
      "source": [
        "#convert to one hot encoding\n",
        "from keras.utils  import to_categorical\n",
        "train_labels = to_categorical(train_labels2)\n",
        "train_labels.shape\n",
        "val_labels = to_categorical(val_labels2)\n",
        "val_labels.shape\n",
        "test_labels = to_categorical(test_labels2)\n",
        "test_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1356, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d-H-X42Uvo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3115bbc-40ae-480d-cf54-ab8d41e116ce"
      },
      "source": [
        "print(train_samples.shape)\n",
        "print(train_labels.shape)\n",
        "print(val_samples.shape)\n",
        "print(val_labels.shape)\n",
        "print(test_samples.shape)\n",
        "print(test_labels2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14590, 9, 500)\n",
            "(14590, 3)\n",
            "(1348, 9, 500)\n",
            "(1348, 3)\n",
            "(1356, 9, 500)\n",
            "(1356,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqxij4vFUvo_"
      },
      "source": [
        "#For CNN and BidirLSTM:\n",
        "number_of_samples = 500\n",
        "train_samples = train_samples.reshape((-1, 9,number_of_samples, 1))\n",
        "val_samples = val_samples.reshape((-1, 9,number_of_samples, 1))\n",
        "test_samples = test_samples.reshape((-1, 9,number_of_samples, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf0eewtJUvpB"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyNXsGAAUvpC"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten, Bidirectional\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.core import Permute, Reshape\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKNc7WZQUvpE"
      },
      "source": [
        "def _data_reshaping(X_tr, X_va, X_ts, network_type):\n",
        "    _, win_len, dim = X_tr.shape\n",
        "    print(network_type)\n",
        "    if network_type=='CNN' or network_type=='ConvLSTM':\n",
        "\n",
        "        X_tr = np.swapaxes(X_tr,1,2)\n",
        "        X_va = np.swapaxes(X_va,1,2)\n",
        "        X_ts = np.swapaxes(X_ts,1,2)\n",
        "\n",
        "        X_tr = np.reshape(X_tr, (-1, dim, win_len, 1))\n",
        "        X_va = np.reshape(X_va, (-1, dim, win_len, 1))\n",
        "        X_ts = np.reshape(X_ts, (-1, dim, win_len, 1))\n",
        "    \n",
        "    return X_tr, X_va, X_ts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcMCdtFUUvpF"
      },
      "source": [
        "from keras import regularizers\n",
        "\n",
        "def model_variant(model, num_feat_map, dim, network_type,p):\n",
        "    print(network_type)\n",
        "    if network_type == 'ConvLSTM':\n",
        "        model.add(Permute((2, 1, 3))) \n",
        "        model.add(Reshape((-1,num_feat_map*dim)))\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=False, stateful=False)))\n",
        "    if network_type == 'CNN':\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(BatchNormalization()) \n",
        "        model.add(Dropout(p))\n",
        "\n",
        "        \n",
        "def model_conv(model, num_feat_map,p,b):\n",
        "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10),    \n",
        "                 activation='relu',\n",
        "                 input_shape=(dim, win_len, 1),\n",
        "                 padding='same'))\n",
        "    \n",
        "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10), activation='relu',padding='same'))\n",
        "    \n",
        "    if (b==1):\n",
        "        model.add(BatchNormalization()) \n",
        "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10), activation='relu',padding='same'))\n",
        "    \n",
        "    if (b==1):\n",
        "        model.add(BatchNormalization()) \n",
        "    model.add(MaxPooling2D(pool_size=(1, 3)))\n",
        "    \n",
        "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10), activation='relu',padding='same')) \n",
        "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10), activation='relu',padding='same'))\n",
        "    if (b==1):\n",
        "        model.add(BatchNormalization()) \n",
        "    model.add(MaxPooling2D(pool_size=(1, 2)))\n",
        "    model.add(Dropout(p))\n",
        "    \n",
        "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10), activation='relu',padding='same'))  \n",
        "    if (b==1):\n",
        "        model.add(BatchNormalization()) \n",
        "    model.add(MaxPooling2D(pool_size=(1, 2)))\n",
        "    \n",
        "    model.add(Dropout(p))\n",
        "    \n",
        "def model_LSTM(model,p):\n",
        "    model.add(LSTM(num_hidden_lstm, \n",
        "               input_shape=(win_len,dim), \n",
        "               return_sequences=True))\n",
        "    model.add(Dropout(p))\n",
        "    model.add(LSTM(num_hidden_lstm, return_sequences=False))\n",
        "    model.add(Dropout(p))\n",
        "    \n",
        "def model_output(model):\n",
        "    model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf8LuDxlUvpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0683401c-6cf2-4530-b55f-1a0456075904"
      },
      "source": [
        "batch_size = 64\n",
        "num_feat_map = 128\n",
        "num_hidden_lstm = 128\n",
        "num_classes = 3\n",
        "\n",
        "\n",
        "network_type = 'ConvLSTM'\n",
        "_, dim, win_len,_ = train_samples.shape\n",
        "\n",
        "print(win_len)\n",
        "print(dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAQRf1DNUvpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96fd4b7-9d53-402e-b8a7-42cdd37110c1"
      },
      "source": [
        "p=0.5 #Dropout\n",
        "b = 1 #BatchNorm\n",
        "print('building the model ... ')\n",
        "model = Sequential()\n",
        "\n",
        "if network_type=='CNN' or network_type=='ConvLSTM':\n",
        "    model_conv(model, num_feat_map,p,b)\n",
        "    model_variant(model, num_feat_map, dim, network_type,p)\n",
        "if network_type=='LSTM':\n",
        "    model_LSTM(model,p)\n",
        "       \n",
        "model_output(model)    \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "building the model ... \n",
            "ConvLSTM\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 9, 500, 128)       1408      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 9, 500, 128)       163968    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 9, 500, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 9, 500, 128)       163968    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 9, 500, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 9, 166, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 9, 166, 128)       163968    \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 9, 166, 128)       163968    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 9, 166, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 9, 83, 128)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 9, 83, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 83, 128)        163968    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 9, 83, 128)        512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 9, 41, 128)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 9, 41, 128)        0         \n",
            "_________________________________________________________________\n",
            "permute (Permute)            (None, 41, 9, 128)        0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 41, 1152)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               1311744   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 2,135,811\n",
            "Trainable params: 2,134,787\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrNCEAD8UvpL"
      },
      "source": [
        "X_train = train_samples\n",
        "y_train = train_labels\n",
        "X_val = val_samples\n",
        "y_val = val_labels\n",
        "X_test = test_samples\n",
        "y_test = test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNsDOBNrUvpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d3aba2-f2ed-4d6d-d0d8-6deb1295e03b"
      },
      "source": [
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14590, 9, 500, 1) (1348, 9, 500, 1) (1356, 9, 500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sDI-pxh6YJr"
      },
      "source": [
        "from keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "class monitor_Training(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        " \n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        " \n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        " \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        y_pred = np.argmax(self.model.predict(X_test), axis=1)\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        print('test accuracy is:', accuracy)\n",
        "        return\n",
        " \n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        " \n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt-rhWxr6YJr"
      },
      "source": [
        "# checkpoint /home/sandeep/data/DCBL_macro_TTV_4.hdf5\n",
        "filepath=\"macro_without_masking.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpsJRPEfUvpP",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ddb5e0-da2b-4a44-9ed3-9dc0e0254fd6"
      },
      "source": [
        "epochs = 40\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "checkpoint2 = monitor_Training()\n",
        "\n",
        "callbacks_list = [checkpoint, checkpoint2]\n",
        "\n",
        "H = model.fit(train_samples, train_labels,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            shuffle=True,\n",
        "            validation_data=(X_val, y_val),\n",
        "             callbacks=callbacks_list\n",
        "             )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "228/228 [==============================] - 3089s 14s/step - loss: 0.7928 - accuracy: 0.6268 - val_loss: 0.9692 - val_accuracy: 0.5697\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.56973, saving model to macro_without_masking.hdf5\n",
            "test accuracy is: 0.5671091445427728\n",
            "Epoch 2/40\n",
            " 80/228 [=========>....................] - ETA: 32:41 - loss: 0.3970 - accuracy: 0.8381"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ASUmtFSUvpQ"
      },
      "source": [
        "history = H"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6yi5aaJUvpS"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "print(np.argmax(np.array(history.history['val_accuracy'])))\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "print('Maximum validation accuracy: ',np.max(np.array(history.history['val_accuracy'])))\n",
        "print('Training accuracy of best model: ',np.array(history.history['accuracy'])[np.argmax(np.array(history.history['val_accuracy']))])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DQyp_5s6YJs"
      },
      "source": [
        "from keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brZIkSmH6YJt"
      },
      "source": [
        "model = load_model(filepath)\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(cf_matrix)\n",
        "class_wise_f1 = np.round(f1_score(y_true, y_pred, average=None)*100)*0.01\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "print('the mean-f1 score: {:.2f}'.format(np.mean(class_wise_f1)))\n",
        "print('accuracy is: {:.2f}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOHjaRCw6YJu"
      },
      "source": [
        "# File by file the Macro comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lHqoVYa6YJu"
      },
      "source": [
        "# load the data windows per file and see their predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8ZBkZMPB6YJv"
      },
      "source": [
        "def get_prediction_accuracy(data_dir, sub_dirs):\n",
        "\n",
        "    files = os.listdir(data_dir+'/left_hip')\n",
        "    number_of_samples = 500\n",
        "\n",
        "    labels_macro =dict()\n",
        "\n",
        "    labels_macro['sandwich'] = 0\n",
        "    labels_macro['fruitsalad'] = 1\n",
        "    labels_macro['cereal'] = 2\n",
        "\n",
        "\n",
        "    #read the labels\n",
        "    labels_loc = 'data/LabelTable.csv'\n",
        "    file_label = open(labels_loc, newline='')\n",
        "    label_reader = csv.reader(file_label)\n",
        "    file_label_mapping = dict()\n",
        "\n",
        "    first = True\n",
        "    for row in label_reader:\n",
        "\n",
        "        if first:\n",
        "            first = False\n",
        "            continue\n",
        "\n",
        "        file_label_mapping[row[0]+'.csv'] = labels_macro[row[1]]\n",
        "\n",
        "\n",
        "    total_count = 0\n",
        "    correct_count = 0\n",
        "\n",
        "    for f in files:\n",
        "\n",
        "        file_data = []\n",
        "        file_label = []\n",
        "\n",
        "        st_index = 0\n",
        "        end_index = 30000\n",
        "        step = 1000 #overlapping window, step: 1000. \n",
        "        window_index = 10000 #6 second window\n",
        "\n",
        "        #print('reading file:',f)\n",
        "        f_name = f\n",
        "\n",
        "        if f_name == '.DS_Store':\n",
        "            continue\n",
        "\n",
        "        total_count = total_count+1\n",
        "\n",
        "        curr_label_file = file_label_mapping[f_name]\n",
        "\n",
        "        while st_index+step < end_index:\n",
        "\n",
        "            data, data_count = parse_IMU_files(data_dir, sub_dirs, st_index, st_index+window_index,  f, number_of_samples)\n",
        "            st_index = st_index+step\n",
        "\n",
        "            if data_count<data_min_count:\n",
        "                continue\n",
        "\n",
        "            train_data_sample  = np.zeros((9, number_of_samples))\n",
        "            train_data_label   = curr_label_file\n",
        "            for i in range(len(data)):\n",
        "                for j in range(len(data[i])):\n",
        "                    train_data_sample[i*3,j]=data[i][j][0]\n",
        "                    train_data_sample[i*3+1,j]=data[i][j][1]\n",
        "                    train_data_sample[i*3+2,j]=data[i][j][2]\n",
        "\n",
        "            file_data.append(train_data_sample)\n",
        "            #file_label.append(train_data_label)\n",
        "\n",
        "        file_data = np.array(file_data)\n",
        "        file_label = curr_label_file\n",
        "\n",
        "        file_data = file_data.reshape((-1, 9,500, 1))\n",
        "\n",
        "        y_pred = np.argmax(model.predict(file_data), axis=1) \n",
        "        counts = np.bincount(y_pred)\n",
        "        prediction = np.argmax(counts) #max occuring value in windows\n",
        "        \n",
        "        #correct prediction\n",
        "        if int(prediction)==int(file_label):\n",
        "            correct_count = correct_count+1\n",
        "        \n",
        "    \n",
        "    return total_count, correct_count        \n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qt4bqJE6YJv"
      },
      "source": [
        "# Test data accuracy:\n",
        "total_count, correct_count = get_prediction_accuracy(data_dir_test, sub_dirs)\n",
        "print('total_count of test files:', total_count)\n",
        "print('correctly predicted test files:', correct_count)\n",
        "print('Percentage accuracy:', (correct_count/total_count)*100.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muRd0lQ96YJw"
      },
      "source": [
        "# Train accuracy\n",
        "total_count, correct_count = get_prediction_accuracy(data_dir_train, sub_dirs)\n",
        "print('total_count of train files:', total_count)\n",
        "print('correctly predicted train files:', correct_count)\n",
        "print('Percentage accuracy:', (correct_count/total_count)*100.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bHqHI0b6YJw"
      },
      "source": [
        "#val accuracy\n",
        "total_count, correct_count = get_prediction_accuracy(data_dir_val, sub_dirs)\n",
        "print('total_count of val files:', total_count)\n",
        "print('correctly predicted val files:', correct_count)\n",
        "print('Percentage accuracy:', (correct_count/total_count)*100.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBJoOl2a6YJw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}